hf_agent:
  ros__parameters:
    # Topic names
    query_topic: "user_query"
    response_topic: "llm_response"
    
    # MCP servers configuration file path
    mcp_servers: "smolagents_mcp.json"
    
    # System prompt template file path
    system_prompt_file: "system_prompt.jinja"
    
    # LLM model name from Hugging Face
    llm_model: "Qwen/Qwen3-0.6B"
    
    # Use 8-bit quantization for the model
    use_int8: true

    # Tool call regex pattern to extract tool calls from LLM response
    tool_call_pattern: "<tool_call>(.*?)</tool_call>"
    
    # LLM generation parameters
    max_new_tokens: 256
    top_k: 10
    temperature: 0.1
    repetition_penalty: 1.1
    do_sample: false
    return_full_result: false
    max_steps: 8
    enable_thinking: false

langgraph_agent_node:
  ros__parameters:
    # Topic names
    query_topic: "user_query"
    response_topic: "llm_response"
    
    # MCP servers configuration file path
    mcp_servers: "langgraph_mcp.json"
    
    # System prompt template file path
    system_prompt_file: "system_prompt.jinja"

    # Model chat template file path
    model_chat_template_file: "qwen3.jinja"
    
    # LLM model name (for Ollama)
    llm_model: "qwen3:0.6b"
    
    # Tool call regex pattern to extract tool calls from LLM response
    tool_call_pattern: "<tool_call>(.*?)</tool_call>"
    
    # Ollama generation parameters
    raw_mode: true
    debug_mode: false
    temperature: 0.0
    repeat_penalty: 1.1
    top_k: 10
    top_p: 0.25
    num_ctx: 8192
    num_predict: 128
    
    # LangGraph workflow parameters
    max_steps: 5
    enable_thinking: false
